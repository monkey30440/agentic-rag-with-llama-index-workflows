import dspy

from config import LLM_MODEL, OPENAI_API_KEY, TEMPERATURE
from planner_trainset import planner_trainset
from workflow import Planner

LM = dspy.LM(model=f"openai/{LLM_MODEL}", temperature=TEMPERATURE, api_key=OPENAI_API_KEY)
dspy.configure(lm=LM)

JUDGE_LM = dspy.LM(model="gpt-4o-mini", temperature=0.0, api_key=OPENAI_API_KEY)
TEACHER_LM = dspy.LM(model="gpt-4o-mini", temperature=0.0, api_key=OPENAI_API_KEY)


class QuerySemanticJudge(dspy.Signature):
    """
    Role: Euro NCAP Technical Retrieval Auditor.
    Objective: Evaluate if the 'pred_query' will retrieve the EXACT same technical content as the 'gold_query' from a vector database.

    You must adhere to the following strict grading rubric:

    1. Score 1.0 (Perfect Match):
       - The technical intent is identical.
       - Synonyms are allowed (e.g., "dummy" = "target", "velocity" = "speed").
       - The query is a noun phrase optimized for search (e.g., "AEB test speed").

    2. Score 0.8 - 0.9 (Minor Noise):
       - Contains harmless filler words (e.g., "requirements for...", "details of...") that do not alter the search scope.
       - The core technical terms are all present.

    3. Score 0.5 - 0.7 (Too Broad / Missing Constraints):
       - Misses a specific value present in Gold (e.g., Gold has "50 km/h", Pred only has "speed").
       - Misses a specific scenario code (e.g., Gold has "CCRs", Pred only has "Car-to-Car").
       - The query is too generic and will return too many irrelevant results.

    4. Score 0.0 - 0.4 (Critical Failure):
       - **NEGATIVE CONSTRAINT VIOLATION**: Contains comparative/temporal words (e.g., "difference", "change", "new", "old", "versus", "comparison").
       - **HALLUCINATION**: Adds constraints not in Gold (e.g., Gold is "AEB", Pred is "AEB at night" -> narrowing the search incorrectly).
       - **WRONG TOPIC**: Retrieves a completely different system (e.g., Gold="LSS", Pred="AEB").
    """

    gold_query: str = dspy.InputField(
        desc="The ground truth query. It is a precise, keyword-optimized noun phrase (e.g., 'CCRs test speed 50 km/h')."
    )

    pred_query: str = dspy.InputField(
        desc="The query generated by the Planner. Needs to be checked for accuracy and forbidden terms."
    )

    critique: str = dspy.OutputField(
        desc="A concise analysis. explicitly point out missing numbers, wrong domains, or forbidden comparative words."
    )

    rating: float = dspy.OutputField(
        desc="A float score between 0.0 and 1.0 (e.g., 1.0, 0.5, 0.0)."
    )


judge = dspy.ChainOfThought(QuerySemanticJudge)


def planner_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):
    gold_tasks = gold.plan.tasks
    pred_tasks = pred.plan.tasks

    ## Basic checks
    if len(gold_tasks) != len(pred_tasks):
        return dspy.Prediction(
            score=0.0,
            feedback=f"Count Mismatch: Expected {len(gold_tasks)}, got {len(pred_tasks)}",
        )

    total_score = 0.0
    total_message = []

    def get_sort_key(task):
        return (
            str(task.mode).strip(),
            str(task.target_date).strip() if task.target_date else "None",
            str(task.target_version).strip() if task.target_version else "None",
            str(task.protocol_type).strip() if task.protocol_type else "None",
            str(task.system_domain).strip() if task.system_domain else "None",
            str(task.rewritten_query).strip(),
        )

    sorted_gold_meta = sorted(gold_tasks, key=get_sort_key)
    sorted_pred_meta = sorted(pred_tasks, key=get_sort_key)

    for i, (gold_task, pred_task) in enumerate(
        zip(sorted_gold_meta, sorted_pred_meta, strict=False)
    ):
        # mode check
        if gold_task.mode != pred_task.mode:
            task_score = 0.0
            task_message = (
                f"Mode Mismatch at Task {i + 1}: Expected {gold_task.mode}, got {pred_task.mode}."
            )
            return dspy.Prediction(score=task_score, feedback=task_message)

        # target date check
        if gold_task.target_date != pred_task.target_date:
            task_score = 0.0
            task_message = f"Target Date Mismatch at Task {i + 1}: Expected {gold_task.target_date}, got {pred_task.target_date}."
            return dspy.Prediction(score=0.0, feedback=task_message)

        # target version check
        if gold_task.target_version != pred_task.target_version:
            task_score = 0.0
            task_message = f"Target Version Mismatch at Task {i + 1}: Expected {gold_task.target_version}, got {pred_task.target_version}."
            return dspy.Prediction(score=task_score, feedback=task_message)

        # protocol type check
        protocol_type_score = 1.0
        if gold_task.protocol_type != pred_task.protocol_type:
            if pred_task.protocol_type is None or str(pred_task.protocol_type) == "None":
                protocol_type_score = 0.5
            else:
                task_score = 0.0
                task_message = f"Protocol Type Mismatch at Task {i + 1}: Expected {gold_task.protocol_type}, got {pred_task.protocol_type}."
                return dspy.Prediction(score=task_score, feedback=task_message)

        # system domain check
        system_domain_score = 1.0
        if gold_task.system_domain != pred_task.system_domain:
            if pred_task.system_domain is None or str(pred_task.system_domain) == "None":
                system_domain_score = 0.5
            else:
                task_score = 0.0
                task_message = f"System Domain Mismatch at Task {i + 1}: Expected {gold_task.system_domain}, got {pred_task.system_domain}."
                return dspy.Prediction(score=task_score, feedback=task_message)

        # Semantic check
        semantic_score = 0.0
        if gold_task.rewritten_query.lower() == pred_task.rewritten_query.lower():
            semantic_score = 1.0
            semantic_critique = "Perfect Match"
        else:
            with dspy.context(lm=JUDGE_LM):
                res = judge(
                    gold_query=gold_task.rewritten_query,
                    pred_query=pred_task.rewritten_query,
                )
            semantic_score = float(res.rating)
            semantic_critique = str(res.critique)

        task_score = protocol_type_score * system_domain_score * semantic_score
        task_message = f"Task {i + 1}: Protocol Type Score: {protocol_type_score}, System Domain Score: {system_domain_score}, Semantic Score: {semantic_score}, Semantic Critique: {semantic_critique}"

        total_score += task_score
        total_message.append(task_message)

    average_score = total_score / len(gold_tasks)
    feedback_message = "; ".join(total_message)

    return dspy.Prediction(score=average_score, feedback=feedback_message)


planner_optimizer = dspy.BootstrapFewShotWithRandomSearch(
    metric=planner_metric_with_feedback,
    teacher_settings=dict(lm=TEACHER_LM),
    max_bootstrapped_demos=4,
    num_candidate_programs=16,
    num_threads=1,
)

optimized_planner = planner_optimizer.compile(Planner(), trainset=planner_trainset)

optimized_planner.save("optimized_planner.json")
print("Optimization complete. Optimized planner saved to optimized_planner.json")
